{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.112:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x119d94ad0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/Users/ameyapatankar/Desktop/SparkCourse/Spark Lynda/Exercise Files/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_path + \"/location_temp.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.format('csv').option('header','true').load(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_date: string (nullable = true)\n",
      " |-- location_id: string (nullable = true)\n",
      " |-- temp_celcius: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_no_header = data_path + '/utilization.csv'\n",
    "df2 = spark.read.format('csv').option('header','false').option('inferSchema','true').load(file_path_no_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+----+---+\n",
      "|                _c0|_c1| _c2| _c3|_c4|\n",
      "+-------------------+---+----+----+---+\n",
      "|03/05/2019 08:06:14|100|0.57|0.51| 47|\n",
      "|03/05/2019 08:11:14|100|0.47|0.62| 43|\n",
      "|03/05/2019 08:16:14|100|0.56|0.57| 62|\n",
      "|03/05/2019 08:21:14|100|0.57|0.56| 50|\n",
      "|03/05/2019 08:26:14|100|0.35|0.46| 43|\n",
      "|03/05/2019 08:31:14|100|0.41|0.58| 48|\n",
      "|03/05/2019 08:36:14|100|0.57|0.35| 58|\n",
      "|03/05/2019 08:41:14|100|0.41| 0.4| 58|\n",
      "|03/05/2019 08:46:14|100|0.53|0.35| 62|\n",
      "|03/05/2019 08:51:14|100|0.51| 0.6| 45|\n",
      "|03/05/2019 08:56:14|100|0.32|0.37| 47|\n",
      "|03/05/2019 09:01:14|100|0.62|0.59| 60|\n",
      "|03/05/2019 09:06:14|100|0.66|0.72| 57|\n",
      "|03/05/2019 09:11:14|100|0.54|0.54| 44|\n",
      "|03/05/2019 09:16:14|100|0.29| 0.4| 47|\n",
      "|03/05/2019 09:21:14|100|0.43|0.68| 66|\n",
      "|03/05/2019 09:26:14|100|0.49|0.66| 65|\n",
      "|03/05/2019 09:31:14|100|0.64|0.55| 66|\n",
      "|03/05/2019 09:36:14|100|0.42| 0.6| 42|\n",
      "|03/05/2019 09:41:14|100|0.55|0.59| 63|\n",
      "+-------------------+---+----+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.withColumnRenamed('_c0','event_datetime') \\\n",
    "         .withColumnRenamed('_c1','server_id')\\\n",
    "         .withColumnRenamed('_c2','cpu_utilization')\\\n",
    "         .withColumnRenamed('_c3','free_memory')\\\n",
    "         .withColumnRenamed('_c4','session_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_counts|\n",
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|            47|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|            43|\n",
      "|03/05/2019 08:16:14|      100|           0.56|       0.57|            62|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|            50|\n",
      "|03/05/2019 08:26:14|      100|           0.35|       0.46|            43|\n",
      "|03/05/2019 08:31:14|      100|           0.41|       0.58|            48|\n",
      "|03/05/2019 08:36:14|      100|           0.57|       0.35|            58|\n",
      "|03/05/2019 08:41:14|      100|           0.41|        0.4|            58|\n",
      "|03/05/2019 08:46:14|      100|           0.53|       0.35|            62|\n",
      "|03/05/2019 08:51:14|      100|           0.51|        0.6|            45|\n",
      "|03/05/2019 08:56:14|      100|           0.32|       0.37|            47|\n",
      "|03/05/2019 09:01:14|      100|           0.62|       0.59|            60|\n",
      "|03/05/2019 09:06:14|      100|           0.66|       0.72|            57|\n",
      "|03/05/2019 09:11:14|      100|           0.54|       0.54|            44|\n",
      "|03/05/2019 09:16:14|      100|           0.29|        0.4|            47|\n",
      "|03/05/2019 09:21:14|      100|           0.43|       0.68|            66|\n",
      "|03/05/2019 09:26:14|      100|           0.49|       0.66|            65|\n",
      "|03/05/2019 09:31:14|      100|           0.64|       0.55|            66|\n",
      "|03/05/2019 09:36:14|      100|           0.42|        0.6|            42|\n",
      "|03/05/2019 09:41:14|      100|           0.55|       0.59|            63|\n",
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data_path = data_path + '/utilization.json'\n",
    "\n",
    "df1_json = spark.read.format('json').load(json_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|           0.77|03/16/2019 17:21:40|       0.22|      115|           58|\n",
      "|           0.53|03/16/2019 17:26:40|       0.23|      115|           64|\n",
      "|            0.6|03/16/2019 17:31:40|       0.19|      115|           82|\n",
      "|           0.46|03/16/2019 17:36:40|       0.32|      115|           60|\n",
      "|           0.77|03/16/2019 17:41:40|       0.49|      115|           84|\n",
      "|           0.62|03/16/2019 17:46:40|       0.31|      115|           73|\n",
      "|           0.71|03/16/2019 17:51:40|       0.54|      115|           67|\n",
      "|           0.67|03/16/2019 17:56:40|       0.54|      115|           83|\n",
      "|           0.72|03/16/2019 18:01:40|       0.26|      115|           68|\n",
      "|           0.62|03/16/2019 18:06:40|       0.52|      115|           60|\n",
      "|           0.58|03/16/2019 18:11:40|       0.23|      115|           60|\n",
      "|           0.51|03/16/2019 18:16:40|       0.35|      115|           62|\n",
      "|           0.54|03/16/2019 18:21:40|       0.33|      115|           78|\n",
      "|           0.84|03/16/2019 18:26:40|       0.35|      115|           66|\n",
      "|           0.65|03/16/2019 18:31:40|       0.51|      115|           89|\n",
      "|            0.8|03/16/2019 18:36:40|       0.25|      115|           76|\n",
      "|           0.66|03/16/2019 18:41:40|       0.41|      115|           87|\n",
      "|           0.67|03/16/2019 18:46:40|       0.36|      115|           62|\n",
      "|           0.63|03/16/2019 18:51:40|       0.54|      115|           67|\n",
      "|           0.51|03/16/2019 18:56:40|       0.51|      115|           58|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_datetime',\n",
       " 'server_id',\n",
       " 'cpu_utilization',\n",
       " 'free_memory',\n",
       " 'session_counts']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_sample = df2.sample(False,fraction= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_sort = df2_sample.sort('event_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_counts|\n",
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "|03/05/2019 08:07:00|      127|           0.52|       0.49|            75|\n",
      "|03/05/2019 08:07:03|      129|            0.6|       0.48|            80|\n",
      "|03/05/2019 08:07:15|      135|           0.55|        0.5|            70|\n",
      "|03/05/2019 08:07:28|      141|           0.69|       0.24|            67|\n",
      "|03/05/2019 08:11:28|      108|            0.8|       0.26|            79|\n",
      "|03/05/2019 08:11:29|      109|           0.67|       0.46|            68|\n",
      "|03/05/2019 08:11:31|      110|           0.58|       0.61|            76|\n",
      "|03/05/2019 08:11:34|      112|           0.78|       0.14|            83|\n",
      "|03/05/2019 08:11:45|      118|           0.55|       0.38|            81|\n",
      "|03/05/2019 08:11:46|      119|           0.23|       0.76|            37|\n",
      "|03/05/2019 08:11:51|      122|            0.5|       0.18|            68|\n",
      "|03/05/2019 08:12:21|      138|           0.54|       0.45|            39|\n",
      "|03/05/2019 08:12:33|      144|           0.54|       0.26|            86|\n",
      "|03/05/2019 08:16:19|      103|           0.61|       0.27|            88|\n",
      "|03/05/2019 08:16:21|      104|           0.57|       0.19|            90|\n",
      "|03/05/2019 08:17:25|      140|           0.48|       0.36|            62|\n",
      "|03/05/2019 08:17:31|      143|           0.51|       0.44|            45|\n",
      "|03/05/2019 08:21:21|      104|            0.8|       0.36|            94|\n",
      "|03/05/2019 08:26:16|      101|           0.95|       0.38|            71|\n",
      "|03/05/2019 08:26:36|      113|           0.63|       0.31|            95|\n",
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2_sort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.filter(df1['location_id'] == 'loc0').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|location_id|count|\n",
      "+-----------+-----+\n",
      "|       loc0| 1000|\n",
      "|       loc1| 1000|\n",
      "|      loc10| 1000|\n",
      "|     loc100| 1000|\n",
      "|     loc101| 1000|\n",
      "|     loc102| 1000|\n",
      "|     loc103| 1000|\n",
      "|     loc104| 1000|\n",
      "|     loc105| 1000|\n",
      "|     loc106| 1000|\n",
      "|     loc107| 1000|\n",
      "|     loc108| 1000|\n",
      "|     loc109| 1000|\n",
      "|      loc11| 1000|\n",
      "|     loc110| 1000|\n",
      "|     loc111| 1000|\n",
      "|     loc112| 1000|\n",
      "|     loc113| 1000|\n",
      "|     loc114| 1000|\n",
      "|     loc115| 1000|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('location_id').count().orderBy('location_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|       loc0|           29.176|\n",
      "|       loc1|           28.246|\n",
      "|      loc10|           25.337|\n",
      "|     loc100|           27.297|\n",
      "|     loc101|           25.317|\n",
      "|     loc102|           30.327|\n",
      "|     loc103|           25.341|\n",
      "|     loc104|           26.204|\n",
      "|     loc105|           26.217|\n",
      "|     loc106|           27.201|\n",
      "|     loc107|           33.268|\n",
      "|     loc108|           32.195|\n",
      "|     loc109|           24.138|\n",
      "|      loc11|           25.308|\n",
      "|     loc110|           26.239|\n",
      "|     loc111|           31.391|\n",
      "|     loc112|           33.359|\n",
      "|     loc113|           30.345|\n",
      "|     loc114|           29.261|\n",
      "|     loc115|           23.239|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.groupBy('location_id').agg({'temp_celcius':'mean'}).orderBy('location_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mExercise Files\u001b[m\u001b[m        Spark Dataframe.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_counts|\n",
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|            47|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|            43|\n",
      "|03/05/2019 08:16:14|      100|           0.56|       0.57|            62|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|            50|\n",
      "|03/05/2019 08:26:14|      100|           0.35|       0.46|            43|\n",
      "|03/05/2019 08:31:14|      100|           0.41|       0.58|            48|\n",
      "|03/05/2019 08:36:14|      100|           0.57|       0.35|            58|\n",
      "|03/05/2019 08:41:14|      100|           0.41|        0.4|            58|\n",
      "|03/05/2019 08:46:14|      100|           0.53|       0.35|            62|\n",
      "|03/05/2019 08:51:14|      100|           0.51|        0.6|            45|\n",
      "|03/05/2019 08:56:14|      100|           0.32|       0.37|            47|\n",
      "|03/05/2019 09:01:14|      100|           0.62|       0.59|            60|\n",
      "|03/05/2019 09:06:14|      100|           0.66|       0.72|            57|\n",
      "|03/05/2019 09:11:14|      100|           0.54|       0.54|            44|\n",
      "|03/05/2019 09:16:14|      100|           0.29|        0.4|            47|\n",
      "|03/05/2019 09:21:14|      100|           0.43|       0.68|            66|\n",
      "|03/05/2019 09:26:14|      100|           0.49|       0.66|            65|\n",
      "|03/05/2019 09:31:14|      100|           0.64|       0.55|            66|\n",
      "|03/05/2019 09:36:14|      100|           0.42|        0.6|            42|\n",
      "|03/05/2019 09:41:14|      100|           0.55|       0.59|            63|\n",
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView('utilization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM utilization WHERE server_id = 115 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_counts|\n",
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "|03/05/2019 08:06:40|      115|           0.78|       0.22|            62|\n",
      "|03/05/2019 08:11:40|      115|            0.6|       0.52|            55|\n",
      "|03/05/2019 08:16:40|      115|           0.55|        0.4|            64|\n",
      "|03/05/2019 08:21:40|      115|           0.51|        0.4|            66|\n",
      "|03/05/2019 08:26:40|      115|           0.48|       0.54|            83|\n",
      "|03/05/2019 08:31:40|      115|           0.72|       0.37|            75|\n",
      "|03/05/2019 08:36:40|      115|           0.58|       0.44|            71|\n",
      "|03/05/2019 08:41:40|      115|           0.58|       0.25|            76|\n",
      "|03/05/2019 08:46:40|      115|           0.74|       0.41|            69|\n",
      "|03/05/2019 08:51:40|      115|           0.74|        0.3|            67|\n",
      "|03/05/2019 08:56:40|      115|           0.64|       0.49|            59|\n",
      "|03/05/2019 09:01:40|      115|           0.67|       0.25|            83|\n",
      "|03/05/2019 09:06:40|      115|           0.48|       0.29|            80|\n",
      "|03/05/2019 09:11:40|      115|           0.57|        0.3|            87|\n",
      "|03/05/2019 09:16:40|      115|           0.71|       0.47|            54|\n",
      "|03/05/2019 09:21:40|      115|           0.54|       0.16|            71|\n",
      "|03/05/2019 09:26:40|      115|           0.58|       0.31|            70|\n",
      "|03/05/2019 09:31:40|      115|           0.69|       0.28|            61|\n",
      "|03/05/2019 09:36:40|      115|           0.52|       0.26|            88|\n",
      "|03/05/2019 09:41:40|      115|           0.48|        0.2|            79|\n",
      "+-------------------+---------+---------------+-----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+-------------------+------+\n",
      "|server_id|count|min(session_counts)|   Avg|\n",
      "+---------+-----+-------------------+------+\n",
      "|      101| 9808|                 71|87.666|\n",
      "|      113| 9418|                 71|86.963|\n",
      "|      145| 9304|                 71|86.977|\n",
      "|      103| 8744|                 71|85.764|\n",
      "|      102| 8586|                 71|85.712|\n",
      "|      133| 8583|                 71|85.467|\n",
      "|      108| 8375|                 71|85.122|\n",
      "|      149| 8288|                 71|84.961|\n",
      "|      137| 8248|                 71|85.006|\n",
      "|      148| 8027|                 71|84.704|\n",
      "|      123| 7918|                 71|84.532|\n",
      "|      118| 7913|                 71|84.658|\n",
      "|      112| 7425|                 71|83.545|\n",
      "|      139| 7383|                 71|83.327|\n",
      "|      104| 7366|                 71|83.346|\n",
      "|      121| 7084|                 71|82.888|\n",
      "|      142| 7084|                 71|82.905|\n",
      "|      146| 7072|                 71|82.951|\n",
      "|      126| 6365|                 71|81.562|\n",
      "|      144| 6220|                 71|81.385|\n",
      "+---------+-----+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT server_id,count(*) as count, min(session_counts), round(avg(session_counts),3) as Avg\\\n",
    "            FROM utilization \\\n",
    "            WHERE session_counts > 70 \\\n",
    "            GROUP BY server_id \\\n",
    "            ORDER BY count DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_df_path = data_path + '/server_name.csv'\n",
    "# df_server = spark.read.csv(csv_df_path, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------------------+-------------------+-------------------+------------------+\n",
      "|summary|     event_datetime|         server_id|    cpu_utilization|        free_memory|    session_counts|\n",
      "+-------+-------------------+------------------+-------------------+-------------------+------------------+\n",
      "|  count|             500000|            500000|             500000|             500000|            500000|\n",
      "|   mean|               null|             124.5| 0.6205177400000115|0.37912809999999625|          69.59616|\n",
      "| stddev|               null|14.430884120553204|0.15875173872912818|0.15830931278376212|14.850676696352831|\n",
      "|    min|03/05/2019 08:06:14|               100|               0.22|                0.0|                32|\n",
      "|    max|04/09/2019 01:22:46|               149|                1.0|               0.78|               105|\n",
      "+-------+-------------------+------------------+-------------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.47047715730807216"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.stat.corr('cpu_utilization','free_memory')#,'session_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+----------------------------+\n",
      "|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev_samp(cpu_utilization)|\n",
      "+---------+--------------------+--------------------+----------------------------+\n",
      "|      148|                0.54|                0.94|         0.11451712518744131|\n",
      "|      137|                0.54|                0.94|         0.11526245077758812|\n",
      "|      133|                0.55|                0.95|         0.11534006553263144|\n",
      "|      108|                0.55|                0.95|         0.11563100171171926|\n",
      "|      101|                 0.6|                 1.0|         0.11651726263197697|\n",
      "|      115|                0.44|                0.84|         0.11569664615014985|\n",
      "|      126|                0.48|                0.88|         0.11542612970702051|\n",
      "|      103|                0.56|                0.96|         0.11617507884178278|\n",
      "|      128|                0.38|                0.78|          0.1153254132405078|\n",
      "|      122|                0.43|                0.83|         0.11563104329209037|\n",
      "|      111|                0.36|                0.76|         0.11530221569464483|\n",
      "|      140|                0.47|                0.87|         0.11539940805020545|\n",
      "|      132|                0.33|                0.73|          0.1145442656350766|\n",
      "|      146|                 0.5|                 0.9|         0.11488129439634706|\n",
      "|      142|                 0.5|                 0.9|         0.11593003726970044|\n",
      "|      139|                0.51|                0.91|         0.11519660023052102|\n",
      "|      120|                0.35|                0.75|         0.11586355920838642|\n",
      "|      117|                0.38|                0.78|         0.11534593941519553|\n",
      "|      112|                0.52|                0.92|         0.11528867845082576|\n",
      "|      127|                0.47|                0.87|         0.11577746913037888|\n",
      "+---------+--------------------+--------------------+----------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT server_id,min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization) \\\n",
    "            FROM utilization \\\n",
    "            GROUP BY server_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "|03/05/2019 08:07:41|      148|           0.85|0.7393840000000045|\n",
      "|03/05/2019 08:12:41|      148|           0.94|0.7393840000000045|\n",
      "|03/05/2019 08:17:41|      148|           0.89|0.7393840000000045|\n",
      "|03/05/2019 08:22:41|      148|           0.74|0.7393840000000045|\n",
      "|03/05/2019 08:27:41|      148|           0.63|0.7393840000000045|\n",
      "|03/05/2019 08:32:41|      148|           0.89|0.7393840000000045|\n",
      "|03/05/2019 08:37:41|      148|           0.77|0.7393840000000045|\n",
      "|03/05/2019 08:42:41|      148|           0.59|0.7393840000000045|\n",
      "|03/05/2019 08:47:41|      148|           0.77|0.7393840000000045|\n",
      "|03/05/2019 08:52:41|      148|           0.71|0.7393840000000045|\n",
      "|03/05/2019 08:57:41|      148|           0.85|0.7393840000000045|\n",
      "|03/05/2019 09:02:41|      148|           0.73|0.7393840000000045|\n",
      "|03/05/2019 09:07:41|      148|           0.89|0.7393840000000045|\n",
      "|03/05/2019 09:12:41|      148|           0.56|0.7393840000000045|\n",
      "|03/05/2019 09:17:41|      148|           0.59|0.7393840000000045|\n",
      "|03/05/2019 09:22:41|      148|           0.78|0.7393840000000045|\n",
      "|03/05/2019 09:27:41|      148|           0.72|0.7393840000000045|\n",
      "|03/05/2019 09:32:41|      148|            0.8|0.7393840000000045|\n",
      "|03/05/2019 09:37:41|      148|           0.55|0.7393840000000045|\n",
      "|03/05/2019 09:42:41|      148|           0.91|0.7393840000000045|\n",
      "+-------------------+---------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT event_datetime,server_id,cpu_utilization, \\\n",
    "            avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_server_util \\\n",
    "            FROM utilization \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|        delta_server|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|03/05/2019 08:07:41|      148|           0.85|0.7393840000000045|  0.1106159999999955|\n",
      "|03/05/2019 08:12:41|      148|           0.94|0.7393840000000045| 0.20061599999999546|\n",
      "|03/05/2019 08:17:41|      148|           0.89|0.7393840000000045| 0.15061599999999553|\n",
      "|03/05/2019 08:22:41|      148|           0.74|0.7393840000000045| 6.15999999995509E-4|\n",
      "|03/05/2019 08:27:41|      148|           0.63|0.7393840000000045|-0.10938400000000448|\n",
      "|03/05/2019 08:32:41|      148|           0.89|0.7393840000000045| 0.15061599999999553|\n",
      "|03/05/2019 08:37:41|      148|           0.77|0.7393840000000045|0.030615999999995536|\n",
      "|03/05/2019 08:42:41|      148|           0.59|0.7393840000000045| -0.1493840000000045|\n",
      "|03/05/2019 08:47:41|      148|           0.77|0.7393840000000045|0.030615999999995536|\n",
      "|03/05/2019 08:52:41|      148|           0.71|0.7393840000000045|-0.02938400000000...|\n",
      "|03/05/2019 08:57:41|      148|           0.85|0.7393840000000045|  0.1106159999999955|\n",
      "|03/05/2019 09:02:41|      148|           0.73|0.7393840000000045| -0.0093840000000045|\n",
      "|03/05/2019 09:07:41|      148|           0.89|0.7393840000000045| 0.15061599999999553|\n",
      "|03/05/2019 09:12:41|      148|           0.56|0.7393840000000045|-0.17938400000000443|\n",
      "|03/05/2019 09:17:41|      148|           0.59|0.7393840000000045| -0.1493840000000045|\n",
      "|03/05/2019 09:22:41|      148|           0.78|0.7393840000000045|0.040615999999995545|\n",
      "|03/05/2019 09:27:41|      148|           0.72|0.7393840000000045|-0.01938400000000451|\n",
      "|03/05/2019 09:32:41|      148|            0.8|0.7393840000000045| 0.06061599999999556|\n",
      "|03/05/2019 09:37:41|      148|           0.55|0.7393840000000045|-0.18938400000000444|\n",
      "|03/05/2019 09:42:41|      148|           0.91|0.7393840000000045| 0.17061599999999555|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT event_datetime,server_id,cpu_utilization, \\\n",
    "            avg(cpu_utilization) OVER (PARTITION BY server_id) as avg_server_util, \\\n",
    "            cpu_utilization - avg(cpu_utilization) OVER (PARTITION BY server_id) as delta_server \\\n",
    "            FROM utilization\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = VectorAssembler(inputCols = ['cpu_utilization','free_memory','session_counts'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_df = vectors.transform(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+--------------+----------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_counts|        features|\n",
      "+-------------------+---------+---------------+-----------+--------------+----------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|            47|[0.57,0.51,47.0]|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|            43|[0.47,0.62,43.0]|\n",
      "|03/05/2019 08:16:14|      100|           0.56|       0.57|            62|[0.56,0.57,62.0]|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|            50|[0.57,0.56,50.0]|\n",
      "|03/05/2019 08:26:14|      100|           0.35|       0.46|            43|[0.35,0.46,43.0]|\n",
      "|03/05/2019 08:31:14|      100|           0.41|       0.58|            48|[0.41,0.58,48.0]|\n",
      "|03/05/2019 08:36:14|      100|           0.57|       0.35|            58|[0.57,0.35,58.0]|\n",
      "|03/05/2019 08:41:14|      100|           0.41|        0.4|            58| [0.41,0.4,58.0]|\n",
      "|03/05/2019 08:46:14|      100|           0.53|       0.35|            62|[0.53,0.35,62.0]|\n",
      "|03/05/2019 08:51:14|      100|           0.51|        0.6|            45| [0.51,0.6,45.0]|\n",
      "|03/05/2019 08:56:14|      100|           0.32|       0.37|            47|[0.32,0.37,47.0]|\n",
      "|03/05/2019 09:01:14|      100|           0.62|       0.59|            60|[0.62,0.59,60.0]|\n",
      "|03/05/2019 09:06:14|      100|           0.66|       0.72|            57|[0.66,0.72,57.0]|\n",
      "|03/05/2019 09:11:14|      100|           0.54|       0.54|            44|[0.54,0.54,44.0]|\n",
      "|03/05/2019 09:16:14|      100|           0.29|        0.4|            47| [0.29,0.4,47.0]|\n",
      "|03/05/2019 09:21:14|      100|           0.43|       0.68|            66|[0.43,0.68,66.0]|\n",
      "|03/05/2019 09:26:14|      100|           0.49|       0.66|            65|[0.49,0.66,65.0]|\n",
      "|03/05/2019 09:31:14|      100|           0.64|       0.55|            66|[0.64,0.55,66.0]|\n",
      "|03/05/2019 09:36:14|      100|           0.42|        0.6|            42| [0.42,0.6,42.0]|\n",
      "|03/05/2019 09:41:14|      100|           0.55|       0.59|            63|[0.55,0.59,63.0]|\n",
      "+-------------------+---------+---------------+-----------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans().setK(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans_361095f20d65"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.setSeed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Cannot resolve column name \"features\" among (event_datetime, server_id, cpu_utilization, free_memory, session_counts);'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o60.apply.\n: org.apache.spark.sql.AnalysisException: Cannot resolve column name \"features\" among (event_datetime, server_id, cpu_utilization, free_memory, session_counts);\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:223)\n\tat org.apache.spark.sql.Dataset$$anonfun$resolve$1.apply(Dataset.scala:223)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.Dataset.resolve(Dataset.scala:222)\n\tat org.apache.spark.sql.Dataset.col(Dataset.scala:1274)\n\tat org.apache.spark.sql.Dataset.apply(Dataset.scala:1241)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-e6b41689c4da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \"\"\"\n\u001b[1;32m   1279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m             \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Cannot resolve column name \"features\" among (event_datetime, server_id, cpu_utilization, free_memory, session_counts);'"
     ]
    }
   ],
   "source": [
    "kmodel = kmeans.fit(df2['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
